{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distSample(numbers, probabilities, rnd_num):\n",
    "    # Sampling a single number from a discrete distribution\n",
    "    #   The possible Numbers in the distribution with their resective\n",
    "    #   Probabilities. rndNum is a randomly drawn probability\n",
    "    #\n",
    "    #   Conditions on Input (not checked):\n",
    "    #   1. Numbers and Probabilites correspond one to one (i.e. first number is\n",
    "    #   drawn w.p. first probability etc). These are numpy arrays.\n",
    "    #   2. rndNum is a number between zero and one.\n",
    "    #   3. Probabilites is a probability vector (numpy array)\n",
    "    # The output is a number (float)\n",
    "\n",
    "    cum_prob = 0\n",
    "    sampled_int = 0\n",
    "    while rnd_num > cum_prob:\n",
    "        cum_prob += probabilities[sampled_int]\n",
    "        sampled_int += 1\n",
    "\n",
    "    return numbers[sampled_int - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CPC18_getDist(H, pH, L, lot_shape, lot_num):\n",
    "    # Extract true full distributions of an option in CPC18\n",
    "    #   input is high outcome (H: int), its probability (pH: double), low outcome\n",
    "    #   (L: int), the shape of the lottery ('-'/'Symm'/'L-skew'/'R-skew' only), and\n",
    "    #   the number of outcomes in the lottery (lot_num: int)\n",
    "    #   output is a matrix (numpy matrix) with first column a list of outcomes (sorted\n",
    "    #   ascending) and the second column their respective probabilities.\n",
    "\n",
    "    if lot_shape == '-':\n",
    "        if pH == 1:\n",
    "            dist = np.array([H, pH])\n",
    "            dist.shape = (1, 2)\n",
    "        else:\n",
    "            dist = np.array([[L, 1-pH], [H, pH]])\n",
    "\n",
    "    else:  # H is multi outcome\n",
    "        # compute H distribution\n",
    "        high_dist = np.zeros(shape=(lot_num, 2))\n",
    "        if lot_shape == 'Symm':\n",
    "            k = lot_num - 1\n",
    "            for i in range(0, lot_num):\n",
    "                high_dist[i, 0] = H - k / 2 + i\n",
    "                high_dist[i, 1] = pH * stats.binom.pmf(i, k, 0.5)\n",
    "\n",
    "        elif (lot_shape == 'R-skew') or (lot_shape == 'L-skew'):\n",
    "            if lot_shape == 'R-skew':\n",
    "                c = -1 - lot_num\n",
    "                dist_sign = 1\n",
    "            else:\n",
    "                c = 1 + lot_num\n",
    "                dist_sign = -1\n",
    "            for i in range(1, lot_num+1):\n",
    "                high_dist[i - 1, 0] = H + c + dist_sign * pow(2, i)\n",
    "                high_dist[i - 1, 1] = pH / pow(2, i)\n",
    "\n",
    "            high_dist[lot_num - 1, 1] = high_dist[lot_num - 1, 1] * 2\n",
    "\n",
    "        # incorporate L into the distribution\n",
    "        dist = np.copy(high_dist)\n",
    "        locb = np.where(high_dist[:, 0] == float(L))\n",
    "        if locb[0].size > 0:\n",
    "            dist[locb, 1] += (1-pH)\n",
    "        elif pH < 1:\n",
    "            dist = np.vstack((dist, [L, 1-pH]))\n",
    "\n",
    "        dist = dist[np.argsort(dist[:, 0])]\n",
    "\n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pBetter(DistX, DistY, corr, accuracy=10000):\n",
    "    # Return probability that a value drawn from DistX is strictly larger than one drawn from DistY\n",
    "    # Input: 2 discrete distributions which are set as matrices of 1st column\n",
    "    # as outcome and 2nd its probability. DistX and DistY are numpy matrices; correlation between the distributions;\n",
    "    # level of accuracy in terms of number of samples to take from distributions\n",
    "    # Output: a list with the estimated probability that X generates value strictly larger than Y, and\n",
    "    # the probability that Y generates value strictly larger than X\n",
    "\n",
    "    nXbetter = 0\n",
    "    nYbetter = 0\n",
    "\n",
    "    for j in range(1, accuracy+1):\n",
    "        rndNum = np.random.uniform(size=2)\n",
    "        sampleX = distSample(DistX[:, 0], DistX[:, 1], rndNum[0])\n",
    "        if corr == 1:\n",
    "            sampleY = distSample(DistY[:, 0], DistY[:, 1], rndNum[0])\n",
    "        elif corr == -1:\n",
    "            sampleY = distSample(DistY[:, 0], DistY[:, 1], 1-rndNum[0])\n",
    "        else:\n",
    "            sampleY = distSample(DistY[:, 0], DistY[:, 1], rndNum[1])\n",
    "\n",
    "        nXbetter = nXbetter + int(sampleX > sampleY)\n",
    "        nYbetter = nYbetter + int(sampleY > sampleX)\n",
    "\n",
    "    pXbetter = nXbetter / accuracy\n",
    "    pYbetter = nYbetter / accuracy\n",
    "\n",
    "    return [pXbetter, pYbetter]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CPC15_isStochasticDom (DistA, DistB):\n",
    "    # Check if one distribution dominates stochastically the other\n",
    "    # Input: 2 discrete distributions which are set as matrices of 1st column\n",
    "    # as outcome and 2nd its probability (DistA and DistB are numpy matrices)\n",
    "    # Output: pandas data framed with 2 columns:\n",
    "    # 'is' a logical output, 'which' a char output ('A', 'B', NaN)\n",
    "\n",
    "    na = DistA.shape[0]\n",
    "    nb = DistB.shape[0]\n",
    "    if np.array_equal(DistA, DistB):\n",
    "        dom = False\n",
    "        which = None\n",
    "    else:\n",
    "        tempa = np.ones(shape=(na, 1))\n",
    "        tempb = np.ones(shape=(nb, 1))\n",
    "        for i in range(0, nb):\n",
    "            sumpa = 0  # DistA(i,2)\n",
    "            j = 0\n",
    "            sumpb = np.sum(DistB[0:i + 1, 1])\n",
    "\n",
    "            while (sumpa != 1) and (j < na) and (sumpa + DistA[j, 1] <= sumpb):\n",
    "                sumpa += DistA[j, 1]\n",
    "                if sumpa == sumpb:\n",
    "                    break\n",
    "                j += 1\n",
    "\n",
    "            if j == na:\n",
    "                j = na - 1\n",
    "            if i == nb:\n",
    "                i = nb - 1\n",
    "\n",
    "            if DistB[i, 0] < DistA[j, 0]:\n",
    "                tempb[i] = 0\n",
    "                break\n",
    "\n",
    "        if np.all(tempb != 0):\n",
    "            dom = True\n",
    "            which = 'B'\n",
    "        else:\n",
    "            for i in range(0, na):\n",
    "                sumpb = 0  # DistA(i,2)\n",
    "                j = 0\n",
    "                sumpa = np.sum(DistA[0: i+1, 1])\n",
    "\n",
    "                while (sumpb != 1) and (j < nb) and (sumpb + DistB[j, 1] <= sumpa):\n",
    "                    sumpb += DistB[j, 1]\n",
    "                    if sumpa == sumpb:\n",
    "                        break\n",
    "                    j += 1\n",
    "\n",
    "                if j == nb:\n",
    "                    j = nb - 1\n",
    "                if i == na:\n",
    "                    i = na - 1\n",
    "\n",
    "                if DistA[i, 0] < DistB[j, 0]:\n",
    "                    tempa[i] = 0\n",
    "                    break\n",
    "\n",
    "            if np.all(tempa != 0):\n",
    "                dom = True\n",
    "                which = 'A'\n",
    "            else:\n",
    "                dom = False\n",
    "                which = None\n",
    "\n",
    "    return pd.DataFrame([{'dom': dom, 'which': which}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CPC15_BEASTsimulation(DistA, DistB, Amb, Corr):\n",
    "    # Simulation of the BEAST model.\n",
    "    #  Input: 2 discrete distributions which are set as matrices of 1st column\n",
    "    # as outcome and 2nd its probability. DistA and DistB are numpy matrices;\n",
    "    #  Amb is a number 1 or 0, this is the ambiguous between the A and B.\n",
    "    #  Corr is thw correlation between A and B, this is a number between -1 to 1.\n",
    "    # Output: numpy array of zise: (nBlocks, 1)\n",
    "\n",
    "    SIGMA = 7\n",
    "    KAPA = 3\n",
    "    BETA = 2.6\n",
    "    GAMA = 0.5\n",
    "    PSI = 0.07\n",
    "    THETA = 1\n",
    "\n",
    "    nTrials = 25\n",
    "    firstFeedback = 6\n",
    "    nBlocks = 5\n",
    "\n",
    "    # draw personal traits\n",
    "    sigma = SIGMA * np.random.uniform(size=1)\n",
    "    kapa = np.random.choice(range(1, KAPA+1), 1)\n",
    "    beta = BETA * np.random.uniform(size=1)\n",
    "    gama = GAMA * np.random.uniform(size=1)\n",
    "    psi = PSI * np.random.uniform(size=1)\n",
    "    theta = THETA * np.random.uniform(size=1)\n",
    "\n",
    "    ObsPay = np.zeros(shape=(nTrials - firstFeedback + 1, 2))  # observed outcomes in A (col1) and B (col2)\n",
    "    Decision = np.empty(shape=(nTrials, 1))\n",
    "    simPred = np.empty(shape=(nBlocks, 1))\n",
    "\n",
    "    # Useful variables\n",
    "    nA = DistA.shape[0]  # num outcomes in A\n",
    "    nB = DistB.shape[0]  # num outcomes in B\n",
    "\n",
    "    if Amb == 1:\n",
    "        ambiguous = True\n",
    "    else:\n",
    "        ambiguous = False\n",
    "\n",
    "    nfeed = 0  # \"t\"; number of outcomes with feedback so far\n",
    "    pBias = np.array([beta / (beta + 1 + pow(nfeed, theta))])\n",
    "    MinA = DistA[0, 0]\n",
    "    MinB = DistB[0, 0]\n",
    "    MaxOutcome = np.maximum(DistA[nA - 1, 0], DistB[nB - 1, 0])\n",
    "    SignMax = np.sign(MaxOutcome)\n",
    "\n",
    "    if MinA == MinB:\n",
    "        RatioMin = 1\n",
    "    elif np.sign(MinA) == np.sign(MinB):\n",
    "        RatioMin = min(abs(MinA), abs(MinB)) / max(abs(MinA), abs(MinB))\n",
    "    else:\n",
    "        RatioMin = 0\n",
    "\n",
    "    Range = MaxOutcome - min(MinA, MinB)\n",
    "    trivial = CPC15_isStochasticDom(DistA, DistB)\n",
    "    BEVa = np.matrix.dot(DistA[:, 0], DistA[:, 1])\n",
    "    if ambiguous:\n",
    "        UEVb = np.matrix.dot(DistB[:, 0], np.repeat([1 / nB], nB))\n",
    "        BEVb = (1-psi) * (UEVb+BEVa) / 2 + psi * MinB\n",
    "        pEstB = np.repeat([float(nB)], 1)  # estimation of probabilties in Amb\n",
    "        t_SPminb = (BEVb - np.mean(DistB[1:nB+1, 0])) / (MinB - np.mean(DistB[1:nB+1, 0]))\n",
    "        if t_SPminb < 0:\n",
    "            pEstB[0] = 0\n",
    "        elif t_SPminb > 1:\n",
    "            pEstB[0] = 1\n",
    "        else:\n",
    "            pEstB[0] = t_SPminb\n",
    "\n",
    "        # Add nb-1 rows to pEstB:\n",
    "        pEstB = np.append(pEstB, np.repeat((1 - pEstB[0]) / (nB - 1), nB-1))\n",
    "\n",
    "    else:\n",
    "        pEstB = DistB[:, 1]\n",
    "        BEVb = np.matrix.dot(DistB[:, 0], pEstB)\n",
    "\n",
    "    # simulation of decisions\n",
    "    for trial in range(nTrials):\n",
    "        STa = 0\n",
    "        STb = 0\n",
    "        # mental simulations\n",
    "        for s in range(1, kapa[0]+1):\n",
    "            rndNum = np.random.uniform(size=2)\n",
    "            if rndNum[0] > pBias[nfeed]:  # Unbiased technique\n",
    "                if nfeed == 0:\n",
    "                    outcomeA = distSample(DistA[:, 0], DistA[:, 1], rndNum[1])\n",
    "                    outcomeB = distSample(DistB[:, 0], pEstB, rndNum[1])\n",
    "                else:\n",
    "                    uniprobs = np.repeat([1 / nfeed], nfeed)\n",
    "                    outcomeA = distSample(ObsPay[0:nfeed, 0], uniprobs, rndNum[1])\n",
    "                    outcomeB = distSample(ObsPay[0:nfeed, 1], uniprobs, rndNum[1])\n",
    "\n",
    "            elif rndNum[0] > (2 / 3) * pBias[nfeed]:  # uniform\n",
    "                outcomeA = distSample(DistA[:, 0], np.repeat([1 / nA], nA), rndNum[1])\n",
    "                outcomeB = distSample(DistB[:, 0], np.repeat([1 / nB], nB), rndNum[1])\n",
    "\n",
    "            elif rndNum[0] > (1 / 3) * pBias[nfeed]:  # contingent pessimism\n",
    "                if SignMax > 0 and RatioMin < gama:\n",
    "                    outcomeA = MinA\n",
    "                    outcomeB = MinB\n",
    "                else:\n",
    "                    outcomeA = distSample(DistA[:, 0], np.repeat([1 / nA], nA), rndNum[1])\n",
    "                    outcomeB = distSample(DistB[:, 0], np.repeat([1 / nB], nB), rndNum[1])\n",
    "\n",
    "            else:  # Sign\n",
    "                if nfeed == 0:\n",
    "                    outcomeA = Range * distSample(np.sign(DistA[:, 0]), DistA[:, 1], rndNum[1])\n",
    "                    outcomeB = Range * distSample(np.sign(DistB[:, 0]), pEstB, rndNum[1])\n",
    "                else:\n",
    "                    uniprobs = np.repeat([1 / nfeed], nfeed)\n",
    "                    outcomeA = Range * distSample(np.sign(ObsPay[0:nfeed, 0]), uniprobs, rndNum[1])\n",
    "                    outcomeB = Range * distSample(np.sign(ObsPay[0:nfeed, 1]), uniprobs, rndNum[1])\n",
    "\n",
    "            STa = STa + outcomeA\n",
    "            STb = STb + outcomeB\n",
    "\n",
    "        STa = STa / kapa\n",
    "        STb = STb / kapa\n",
    "\n",
    "        # error term\n",
    "        if trivial['dom'][0]:\n",
    "            error = 0\n",
    "        else:\n",
    "            error = sigma * np.random.normal(size=1)  # positive values contribute to attraction to A\n",
    "\n",
    "        # decision\n",
    "        Decision[trial] = (BEVa - BEVb) + (STa - STb) + error < 0\n",
    "        if (BEVa - BEVb) + (STa - STb) + error == 0:\n",
    "            Decision[trial] = np.random.choice(range(1, 3), size=1, replace=False) - 1\n",
    "\n",
    "        if trial >= firstFeedback - 1:\n",
    "            #  got feedback\n",
    "            nfeed += 1\n",
    "            pBias = np.append(pBias, beta / (beta + 1 + pow(nfeed, theta)))\n",
    "            rndNumObs = np.random.uniform(size=1)\n",
    "            ObsPay[nfeed - 1, 0] = distSample(DistA[:, 0], DistA[:, 1], rndNumObs)  # draw outcome from A\n",
    "            if Corr == 1:\n",
    "                ObsPay[nfeed - 1, 1] = distSample(DistB[:, 0], DistB[:, 1], rndNumObs)\n",
    "            elif Corr == -1:\n",
    "                ObsPay[nfeed - 1, 1] = distSample(DistB[:, 0], DistB[:, 1], 1-rndNumObs)\n",
    "            else:\n",
    "                # draw outcome from B\n",
    "                ObsPay[nfeed - 1, 1] = distSample(DistB[:, 0], DistB[:, 1], np.random.uniform(size=1))\n",
    "            if ambiguous:\n",
    "                BEVb = (1 - 1 / (nTrials-firstFeedback+1)) * BEVb + 1 / (nTrials-firstFeedback+1) * ObsPay[nfeed - 1, 1]\n",
    "\n",
    "    # compute B-rates for this simulation\n",
    "    blockSize = nTrials / nBlocks\n",
    "    for b in range(1, nBlocks+1):\n",
    "        simPred[b-1] = np.mean(Decision[int(((b - 1) * blockSize + 1)-1):int(b * blockSize)])\n",
    "\n",
    "    return simPred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CPC15_BEASTpred(Ha, pHa, La, LotShapeA, LotNumA, Hb, pHb, Lb, LotShapeB, LotNumB, Amb, Corr):\n",
    "    # Prediction of (the original) BEAST model for one problem\n",
    "    # Input: for a and b: high outcome (Ha/ Hb: int), its probability (pHa/ pHb: double), low outcome\n",
    "    #  (La/ Lb: int), the shape of the lottery (LotShapeA/ LotShapeB that can be:'-'/'Symm'/'L-skew'/'R-skew' only),\n",
    "    #  the number of outcomes in the lottery (lot_numA/ LotNumB: int),\n",
    "    #  Amb indicates if B is ambiguous (=1) or not (=0).\n",
    "    #  Corr is the correlation between A and B, this is a number between -1 to 1.\n",
    "    # Output: is the prediction of the BEAST model: this is a numpy of size (5,1)\n",
    "\n",
    "    Prediction = np.repeat([0], 5)\n",
    "    Prediction.shape = (5, 1)\n",
    "\n",
    "    # get both options' distributions\n",
    "    DistA = CPC18_getDist(Ha, pHa, La, LotShapeA, LotNumA)\n",
    "    DistB = CPC18_getDist(Hb, pHb, Lb, LotShapeB, LotNumB)\n",
    "\n",
    "    # run model simulation nSims times\n",
    "    nSims = 4000\n",
    "    for sim in range(0, nSims):\n",
    "        simPred = CPC15_BEASTsimulation(DistA, DistB, Amb, Corr)\n",
    "        Prediction = np.add(Prediction, (1 / nSims) * simPred)\n",
    "\n",
    "    return Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PF_Features(Ha, pHa, La, LotShapeA, LotNumA, Hb, pHb, Lb, LotShapeB, LotNumB, Amb, Corr):\n",
    "    # Finds the values of the engineered features that are part of Psychological Forest\n",
    "    # Gets as input the parameters defining the choice problem in CPC18 and returns\n",
    "    # as output a pandas data frame with this problem's features\n",
    "\n",
    "    # Compute \"naive\" and \"psychological\" features as per Plonsky, Erev, Hazan, and Tennenholtz, 2017\n",
    "    DistA = CPC18_getDist(Ha, pHa, La, LotShapeA, LotNumA)\n",
    "    DistB = CPC18_getDist(Hb, pHb, Lb, LotShapeB, LotNumB)\n",
    "    diffEV = (np.matrix.dot(DistB[:, 0], DistB[:, 1]) - np.matrix.dot(DistA[:, 0], DistA[:, 1]))\n",
    "    diffSDs = (getSD(DistB[:, 0], DistB[:, 1]) - getSD(DistA[:, 0], DistA[:, 1]))\n",
    "    MinA = DistA[0, 0]\n",
    "    MinB = DistB[0, 0]\n",
    "    diffMins = MinB - MinA\n",
    "    nA = DistA.shape[0]  # num outcomes in A\n",
    "    nB = DistB.shape[0]  # num outcomes in B\n",
    "    MaxA = DistA[nA - 1, 0]\n",
    "    MaxB = DistB[nB - 1, 0]\n",
    "    diffMaxs = MaxB - MaxA\n",
    "\n",
    "    diffUV = (np.matrix.dot(DistB[:, 0], np.repeat([1 / nB], nB))) - (np.matrix.dot(DistA[:, 0], np.repeat([1 / nA], nA)))\n",
    "    if Amb == 1:\n",
    "        ambiguous = True\n",
    "    else:\n",
    "        ambiguous = False\n",
    "\n",
    "    MaxOutcome = max(MaxA, MaxB)\n",
    "    SignMax = np.sign(MaxOutcome)\n",
    "    if MinA == MinB:\n",
    "        RatioMin = 1\n",
    "    elif np.sign(MinA) == np.sign(MinB):\n",
    "        RatioMin = min(abs(MinA), abs(MinB)) / max(abs(MinA), abs(MinB))\n",
    "    else:\n",
    "        RatioMin = 0\n",
    "\n",
    "    Range = MaxOutcome - min(MinA, MinB)\n",
    "    diffSignEV = (Range * np.matrix.dot(np.sign(DistB[:, 0]), DistB[:, 1]) -\n",
    "                  Range * np.matrix.dot(np.sign(DistA[:, 0]), DistA[:, 1]))\n",
    "    trivial = CPC15_isStochasticDom(DistA, DistB)\n",
    "    whchdom = trivial['which'][0]\n",
    "    Dom = 0\n",
    "    if trivial['dom'][0] and whchdom == 'A':\n",
    "        Dom = -1\n",
    "    if trivial['dom'][0] and whchdom == 'B':\n",
    "        Dom = 1\n",
    "    BEVa = np.matrix.dot(DistA[:, 0], DistA[:, 1])\n",
    "    if ambiguous:\n",
    "        UEVb = np.matrix.dot(DistB[:, 0], np.repeat(1 / nB, nB))\n",
    "        BEVb = (UEVb + BEVa + MinB) / 3\n",
    "        pEstB = np.repeat([float(nB)], 1)  # estimation of probabilties in Amb\n",
    "        t_SPminb = (BEVb - np.mean(DistB[1:nB + 1, 0])) / (MinB - np.mean(DistB[1:nB + 1, 0]))\n",
    "        if t_SPminb < 0:\n",
    "            pEstB[0] = 0\n",
    "        elif t_SPminb > 1:\n",
    "            pEstB[0] = 1\n",
    "        else:\n",
    "            pEstB[0] = t_SPminb\n",
    "        pEstB = np.append(pEstB, np.repeat([(1 - pEstB[0]) / (nB - 1)], nB - 1))\n",
    "    else:\n",
    "        pEstB = DistB[:, 1]\n",
    "        BEVb = np.matrix.dot(DistB[:, 0], pEstB)\n",
    "\n",
    "    diffBEV0 = (BEVb - BEVa)\n",
    "    BEVfb = (BEVb + (np.matrix.dot(DistB[:, 0], DistB[:, 1]))) / 2\n",
    "    diffBEVfb = (BEVfb - BEVa)\n",
    "\n",
    "    sampleDistB = np.column_stack((DistB[:, 0], pEstB))\n",
    "    probsBetter = get_pBetter(DistA, sampleDistB, corr=1)\n",
    "    pAbetter = probsBetter[0]\n",
    "    pBbetter = probsBetter[1]\n",
    "    pBbet_Unbiased1 = pBbetter - pAbetter\n",
    "\n",
    "    sampleUniDistA = np.column_stack((DistA[:, 0], np.repeat([1 / nA], nA)))\n",
    "    sampleUniDistB = np.column_stack((DistB[:, 0], np.repeat([1 / nB], nB)))\n",
    "    probsBetterUni = get_pBetter(sampleUniDistA, sampleUniDistB, corr=1)\n",
    "    pBbet_Uniform = probsBetterUni[1] - probsBetterUni[0]\n",
    "\n",
    "    sampleSignA = np.copy(DistA)\n",
    "    sampleSignA[:, 0] = np.sign(sampleSignA[:, 0])\n",
    "    sampleSignB = np.column_stack((np.sign(DistB[:, 0]), pEstB))\n",
    "    probsBetterSign = get_pBetter(sampleSignA, sampleSignB, corr=1)\n",
    "    pBbet_Sign1 = probsBetterSign[1] - probsBetterSign[0]\n",
    "    sampleSignBFB = np.column_stack((np.sign(DistB[:, 0]), DistB[:, 1]))\n",
    "    if Corr == 1:\n",
    "        probsBetter = get_pBetter(DistA, DistB, corr=1)\n",
    "        probsBetterSign = get_pBetter(sampleSignA, sampleSignBFB, corr=1)\n",
    "    elif Corr == -1:\n",
    "        probsBetter = get_pBetter(DistA, DistB, corr=-1)\n",
    "        probsBetterSign = get_pBetter(sampleSignA, sampleSignBFB, corr=-1)\n",
    "    else:\n",
    "        probsBetter = get_pBetter(DistA, DistB, corr=0)\n",
    "        probsBetterSign = get_pBetter(sampleSignA, sampleSignBFB, corr=0)\n",
    "\n",
    "    pBbet_UnbiasedFB = probsBetter[1] - probsBetter[0]\n",
    "    pBbet_SignFB = probsBetterSign[1] - probsBetterSign[0]\n",
    "\n",
    "    # convert lot shape: '-'/'Symm'/'L-skew'/'R-skew' to 4 different features for the RF model\n",
    "    lot_shape_listA = lot_shape_convert(LotShapeA)\n",
    "    lot_shape_listB = lot_shape_convert(LotShapeB)\n",
    "\n",
    "    # create features data frame\n",
    "    feats_labels = ('Ha', 'pHa', 'La', 'lot_shape__A', 'lot_shape_symm_A', 'lot_shape_L_A', 'lot_shape_R_A', 'LotNumA',\n",
    "                    'Hb', 'pHb', 'Lb', 'lot_shape__B', 'lot_shape_symm_B', 'lot_shape_L_B', 'lot_shape_R_B', 'LotNumB',\n",
    "                    'Amb', 'Corr', 'diffEV', 'diffSDs', 'diffMins', 'diffMaxs', 'diffUV', 'RatioMin', 'SignMax',\n",
    "                    'pBbet_Unbiased1', 'pBbet_UnbiasedFB', 'pBbet_Uniform', 'pBbet_Sign1', 'pBbet_SignFB', 'Dom',\n",
    "                    'diffBEV0', 'diffBEVfb', 'diffSignEV')\n",
    "    data_lists = [[Ha, pHa, La], lot_shape_listA, [LotNumA, Hb, pHb, Lb], lot_shape_listB, [LotNumB, Amb, Corr,\n",
    "                             diffEV, diffSDs, diffMins, diffMaxs, diffUV, RatioMin, SignMax, pBbet_Unbiased1,\n",
    "                             pBbet_UnbiasedFB, pBbet_Uniform, pBbet_Sign1, pBbet_SignFB, Dom, diffBEV0,\n",
    "                             diffBEVfb, diffSignEV]]\n",
    "    features_data = [item for sublist in data_lists for item in sublist]\n",
    "    tmpFeats = pd.DataFrame(features_data, index=feats_labels).T\n",
    "\n",
    "    # duplicate features data frame as per number of blocks\n",
    "    Feats = pd.concat([tmpFeats] * 5)\n",
    "\n",
    "    # get BEAST model prediction as feature\n",
    "    beastPs = CPC15_BEASTpred(Ha, pHa, La, LotShapeA, LotNumA, Hb, pHb, Lb, LotShapeB, LotNumB, Amb, Corr)\n",
    "    Feats['BEASTpred'] = beastPs\n",
    "\n",
    "    Feats['block'] = np.arange(1, 6)\n",
    "    Feats['Feedback'] = 1\n",
    "    Feats.loc[Feats['block'] == 1, 'Feedback'] = 0\n",
    "\n",
    "    return Feats\n",
    "\n",
    "\n",
    "# To compute the distribution's standard deviation\n",
    "def getSD(vals, probs):\n",
    "    m = np.matrix.dot(vals, probs.T)\n",
    "    sqds = np.power((vals - m), 2)\n",
    "    var = np.matrix.dot(probs, sqds.T)\n",
    "    return math.sqrt(var)\n",
    "\n",
    "\n",
    "# Convert lot shape feautre to vector of 4 features\n",
    "def lot_shape_convert(lot_shape):\n",
    "    return {\n",
    "        '-': [1, 0, 0, 0],\n",
    "        'Symm': [0, 1, 0, 0],\n",
    "        'L-skew': [0, 0, 1, 0],\n",
    "        'R-skew': [0, 0, 0, 1],\n",
    "    }[lot_shape]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def CPC18_PF_pred(train_data, Ha, pHa, La, LotShapeA, LotNumA, Hb, pHb, Lb, LotShapeB, LotNumB, Amb, Corr):\n",
    "    # Prediction of Psychological Forest for one problem\n",
    "    #\n",
    "    #  This function gets as input 12 parameters which define a problem in CPC18\n",
    "    #  and outputs Psych. Forest's prediction in that problem for five blocks of\n",
    "    #  five trials each (the first is without and the others are with feedback\n",
    "\n",
    "    # get data frame of engineered features's values for the prediction problem\n",
    "    # Output: the prediction of 5 iteration of the game (numpy array of shape(1,5))\n",
    "    Feats = get_PF_Features(Ha, pHa, La, LotShapeA, LotNumA, Hb, pHb, Lb, LotShapeB, LotNumB, Amb, Corr)\n",
    "    Feats = Feats.values\n",
    "    \n",
    "    n_runs = 10\n",
    "    prediction = np.repeat([0], 5)\n",
    "    prediction.shape = (1, 5)\n",
    "    for run in range(n_runs):\n",
    "        # train a random forest algorithm using all supplied features of the train data\n",
    "        x_train = train_data.iloc[:, 1:38]\n",
    "        y_train = train_data['B_rate']\n",
    "        mlp_model = RandomForestRegressor(n_estimators=500, max_features=0.3333, min_samples_leaf=5)\n",
    "        mlp_model.fit(X=x_train, y=y_train)\n",
    "        # let the trained RF predict the prediction prbolem\n",
    "        pred = mlp_model.predict(Feats)\n",
    "        prediction = np.add(prediction, (1 / n_runs) * pred)\n",
    "\n",
    "    return prediction, mlp_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODIFICATION: Here I have tried to leverage tranfer learning by pretraining the model on Choice13K dataset and fine-tuning the model with CPC-18 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CPC18_PF_pred_new(train_data, Ha, pHa, La, LotShapeA, LotNumA, Hb, pHb, Lb, LotShapeB, LotNumB, Amb, Corr, mlp_model):\n",
    "    Feats = get_PF_Features(Ha, pHa, La, LotShapeA, LotNumA, Hb, pHb, Lb, LotShapeB, LotNumB, Amb, Corr)\n",
    "    \n",
    "    n_runs = 10\n",
    "    prediction = np.repeat([0], 5)\n",
    "    prediction.shape = (1, 5)\n",
    "    for run in range(n_runs):\n",
    "        x_train = train_data.iloc[:, 1:38]\n",
    "        y_train = train_data['B_rate']\n",
    "        # mlp_model.n_estimators += 100\n",
    "        mlp_model.fit(X=x_train, y=y_train)\n",
    "        pred = mlp_model.predict(Feats)\n",
    "        # errors.append(log_loss(y_train, pred))\n",
    "        prediction = np.add(prediction, (1 / n_runs) * pred)\n",
    "\n",
    "    return prediction, mlp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "C13k_Data = pd.read_csv('c13k_selections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7787914015111279"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "pretrained_model = MLPRegressor(max_iter=1000, hidden_layer_sizes=[200, 275, 100], warm_start=True)\n",
    "x_train = C13k_Data.iloc[:, 1:14]\n",
    "y_train = C13k_Data['bRate']\n",
    "pretrained_model.fit(X=x_train, y=y_train)\n",
    "pretrained_model.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv('CPC18_EstSet.csv')\n",
    "train_data = pd.read_csv('TrainData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "nProblems = Data.shape[0]\n",
    "PredictedAll = np.zeros(shape=(nProblems, 5))\n",
    "\n",
    "# errors = []\n",
    "mlp_model = Pipeline([\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('pca', PCA(n_components=13)),\n",
    "    ('mlp', pretrained_model)\n",
    "])\n",
    "# mlp_model = RandomForestRegressor(n_estimators=500, max_features=0.3333, min_samples_leaf=5, warm_start=True)\n",
    "for prob in range(nProblems):\n",
    "    # read problem's parameters\n",
    "    Ha = Data['Ha'][prob]\n",
    "    pHa = Data['pHa'][prob]\n",
    "    La = Data['La'][prob]\n",
    "    LotShapeA = Data['LotShapeA'][prob]\n",
    "    LotNumA = Data['LotNumA'][prob]\n",
    "    Hb = Data['Hb'][prob]\n",
    "    pHb = Data['pHb'][prob]\n",
    "    Lb = Data['Lb'][prob]\n",
    "    LotShapeB = Data['LotShapeB'][prob]\n",
    "    LotNumB = Data['LotNumB'][prob]\n",
    "    Amb = Data['Amb'][prob]\n",
    "    Corr = Data['Corr'][prob]\n",
    "    Prediction, mlp_model = CPC18_PF_pred_new(train_data, Ha, pHa, La, LotShapeA, LotNumA, Hb, pHb, Lb, LotShapeB, LotNumB, Amb,\n",
    "                               Corr, mlp_model)\n",
    "    PredictedAll[prob, :] = Prediction\n",
    "    # print('{}: Finish problem number: {}'.format((time.asctime(time.localtime(time.time()))), prob + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE over the 60 problems: 1.518164884135073\n"
     ]
    }
   ],
   "source": [
    "ObservedAll = Data[['B.1', 'B.2', 'B.3', 'B.4', 'B.5']]\n",
    "probMSEs = 100 * ((PredictedAll - ObservedAll) ** 2).mean(axis=1)\n",
    "totalMSE = np.mean(probMSEs)\n",
    "print('MSE over the {} problems: {}'.format(nProblems, totalMSE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
