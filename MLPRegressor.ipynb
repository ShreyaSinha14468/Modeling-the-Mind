{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distSample(numbers, probabilities, rnd_num):\n",
    "    # Sampling a single number from a discrete distribution\n",
    "    #   The possible Numbers in the distribution with their resective\n",
    "    #   Probabilities. rndNum is a randomly drawn probability\n",
    "    #\n",
    "    #   Conditions on Input (not checked):\n",
    "    #   1. Numbers and Probabilites correspond one to one (i.e. first number is\n",
    "    #   drawn w.p. first probability etc). These are numpy arrays.\n",
    "    #   2. rndNum is a number between zero and one.\n",
    "    #   3. Probabilites is a probability vector (numpy array)\n",
    "    # The output is a number (float)\n",
    "\n",
    "    cum_prob = 0\n",
    "    sampled_int = 0\n",
    "    while rnd_num > cum_prob:\n",
    "        cum_prob += probabilities[sampled_int]\n",
    "        sampled_int += 1\n",
    "\n",
    "    return numbers[sampled_int - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CPC18_getDist(H, pH, L, lot_shape, lot_num):\n",
    "    # Extract true full distributions of an option in CPC18\n",
    "    #   input is high outcome (H: int), its probability (pH: double), low outcome\n",
    "    #   (L: int), the shape of the lottery ('-'/'Symm'/'L-skew'/'R-skew' only), and\n",
    "    #   the number of outcomes in the lottery (lot_num: int)\n",
    "    #   output is a matrix (numpy matrix) with first column a list of outcomes (sorted\n",
    "    #   ascending) and the second column their respective probabilities.\n",
    "\n",
    "    if lot_shape == '-':\n",
    "        if pH == 1:\n",
    "            dist = np.array([H, pH])\n",
    "            dist.shape = (1, 2)\n",
    "        else:\n",
    "            dist = np.array([[L, 1-pH], [H, pH]])\n",
    "\n",
    "    else:  # H is multi outcome\n",
    "        # compute H distribution\n",
    "        high_dist = np.zeros(shape=(lot_num, 2))\n",
    "        if lot_shape == 'Symm':\n",
    "            k = lot_num - 1\n",
    "            for i in range(0, lot_num):\n",
    "                high_dist[i, 0] = H - k / 2 + i\n",
    "                high_dist[i, 1] = pH * stats.binom.pmf(i, k, 0.5)\n",
    "\n",
    "        elif (lot_shape == 'R-skew') or (lot_shape == 'L-skew'):\n",
    "            if lot_shape == 'R-skew':\n",
    "                c = -1 - lot_num\n",
    "                dist_sign = 1\n",
    "            else:\n",
    "                c = 1 + lot_num\n",
    "                dist_sign = -1\n",
    "            for i in range(1, lot_num+1):\n",
    "                high_dist[i - 1, 0] = H + c + dist_sign * pow(2, i)\n",
    "                high_dist[i - 1, 1] = pH / pow(2, i)\n",
    "\n",
    "            high_dist[lot_num - 1, 1] = high_dist[lot_num - 1, 1] * 2\n",
    "\n",
    "        # incorporate L into the distribution\n",
    "        dist = np.copy(high_dist)\n",
    "        locb = np.where(high_dist[:, 0] == float(L))\n",
    "        if locb[0].size > 0:\n",
    "            dist[locb, 1] += (1-pH)\n",
    "        elif pH < 1:\n",
    "            dist = np.vstack((dist, [L, 1-pH]))\n",
    "\n",
    "        dist = dist[np.argsort(dist[:, 0])]\n",
    "\n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pBetter(DistX, DistY, corr, accuracy=10000):\n",
    "    # Return probability that a value drawn from DistX is strictly larger than one drawn from DistY\n",
    "    # Input: 2 discrete distributions which are set as matrices of 1st column\n",
    "    # as outcome and 2nd its probability. DistX and DistY are numpy matrices; correlation between the distributions;\n",
    "    # level of accuracy in terms of number of samples to take from distributions\n",
    "    # Output: a list with the estimated probability that X generates value strictly larger than Y, and\n",
    "    # the probability that Y generates value strictly larger than X\n",
    "\n",
    "    nXbetter = 0\n",
    "    nYbetter = 0\n",
    "\n",
    "    for j in range(1, accuracy+1):\n",
    "        rndNum = np.random.uniform(size=2)\n",
    "        sampleX = distSample(DistX[:, 0], DistX[:, 1], rndNum[0])\n",
    "        if corr == 1:\n",
    "            sampleY = distSample(DistY[:, 0], DistY[:, 1], rndNum[0])\n",
    "        elif corr == -1:\n",
    "            sampleY = distSample(DistY[:, 0], DistY[:, 1], 1-rndNum[0])\n",
    "        else:\n",
    "            sampleY = distSample(DistY[:, 0], DistY[:, 1], rndNum[1])\n",
    "\n",
    "        nXbetter = nXbetter + int(sampleX > sampleY)\n",
    "        nYbetter = nYbetter + int(sampleY > sampleX)\n",
    "\n",
    "    pXbetter = nXbetter / accuracy\n",
    "    pYbetter = nYbetter / accuracy\n",
    "\n",
    "    return [pXbetter, pYbetter]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CPC15_isStochasticDom (DistA, DistB):\n",
    "    # Check if one distribution dominates stochastically the other\n",
    "    # Input: 2 discrete distributions which are set as matrices of 1st column\n",
    "    # as outcome and 2nd its probability (DistA and DistB are numpy matrices)\n",
    "    # Output: pandas data framed with 2 columns:\n",
    "    # 'is' a logical output, 'which' a char output ('A', 'B', NaN)\n",
    "\n",
    "    na = DistA.shape[0]\n",
    "    nb = DistB.shape[0]\n",
    "    if np.array_equal(DistA, DistB):\n",
    "        dom = False\n",
    "        which = None\n",
    "    else:\n",
    "        tempa = np.ones(shape=(na, 1))\n",
    "        tempb = np.ones(shape=(nb, 1))\n",
    "        for i in range(0, nb):\n",
    "            sumpa = 0  # DistA(i,2)\n",
    "            j = 0\n",
    "            sumpb = np.sum(DistB[0:i + 1, 1])\n",
    "\n",
    "            while (sumpa != 1) and (j < na) and (sumpa + DistA[j, 1] <= sumpb):\n",
    "                sumpa += DistA[j, 1]\n",
    "                if sumpa == sumpb:\n",
    "                    break\n",
    "                j += 1\n",
    "\n",
    "            if j == na:\n",
    "                j = na - 1\n",
    "            if i == nb:\n",
    "                i = nb - 1\n",
    "\n",
    "            if DistB[i, 0] < DistA[j, 0]:\n",
    "                tempb[i] = 0\n",
    "                break\n",
    "\n",
    "        if np.all(tempb != 0):\n",
    "            dom = True\n",
    "            which = 'B'\n",
    "        else:\n",
    "            for i in range(0, na):\n",
    "                sumpb = 0  # DistA(i,2)\n",
    "                j = 0\n",
    "                sumpa = np.sum(DistA[0: i+1, 1])\n",
    "\n",
    "                while (sumpb != 1) and (j < nb) and (sumpb + DistB[j, 1] <= sumpa):\n",
    "                    sumpb += DistB[j, 1]\n",
    "                    if sumpa == sumpb:\n",
    "                        break\n",
    "                    j += 1\n",
    "\n",
    "                if j == nb:\n",
    "                    j = nb - 1\n",
    "                if i == na:\n",
    "                    i = na - 1\n",
    "\n",
    "                if DistA[i, 0] < DistB[j, 0]:\n",
    "                    tempa[i] = 0\n",
    "                    break\n",
    "\n",
    "            if np.all(tempa != 0):\n",
    "                dom = True\n",
    "                which = 'A'\n",
    "            else:\n",
    "                dom = False\n",
    "                which = None\n",
    "\n",
    "    return pd.DataFrame([{'dom': dom, 'which': which}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CPC15_BEASTsimulation(DistA, DistB, Amb, Corr):\n",
    "    # Simulation of the BEAST model.\n",
    "    #  Input: 2 discrete distributions which are set as matrices of 1st column\n",
    "    # as outcome and 2nd its probability. DistA and DistB are numpy matrices;\n",
    "    #  Amb is a number 1 or 0, this is the ambiguous between the A and B.\n",
    "    #  Corr is thw correlation between A and B, this is a number between -1 to 1.\n",
    "    # Output: numpy array of zise: (nBlocks, 1)\n",
    "\n",
    "    SIGMA = 7\n",
    "    KAPA = 3\n",
    "    BETA = 2.6\n",
    "    GAMA = 0.5\n",
    "    PSI = 0.07\n",
    "    THETA = 1\n",
    "\n",
    "    nTrials = 25\n",
    "    firstFeedback = 6\n",
    "    nBlocks = 5\n",
    "\n",
    "    # draw personal traits\n",
    "    sigma = SIGMA * np.random.uniform(size=1)\n",
    "    kapa = np.random.choice(range(1, KAPA+1), 1)\n",
    "    beta = BETA * np.random.uniform(size=1)\n",
    "    gama = GAMA * np.random.uniform(size=1)\n",
    "    psi = PSI * np.random.uniform(size=1)\n",
    "    theta = THETA * np.random.uniform(size=1)\n",
    "\n",
    "    ObsPay = np.zeros(shape=(nTrials - firstFeedback + 1, 2))  # observed outcomes in A (col1) and B (col2)\n",
    "    Decision = np.empty(shape=(nTrials, 1))\n",
    "    simPred = np.empty(shape=(nBlocks, 1))\n",
    "\n",
    "    # Useful variables\n",
    "    nA = DistA.shape[0]  # num outcomes in A\n",
    "    nB = DistB.shape[0]  # num outcomes in B\n",
    "\n",
    "    if Amb == 1:\n",
    "        ambiguous = True\n",
    "    else:\n",
    "        ambiguous = False\n",
    "\n",
    "    nfeed = 0  # \"t\"; number of outcomes with feedback so far\n",
    "    pBias = np.array([beta / (beta + 1 + pow(nfeed, theta))])\n",
    "    MinA = DistA[0, 0]\n",
    "    MinB = DistB[0, 0]\n",
    "    MaxOutcome = np.maximum(DistA[nA - 1, 0], DistB[nB - 1, 0])\n",
    "    SignMax = np.sign(MaxOutcome)\n",
    "\n",
    "    if MinA == MinB:\n",
    "        RatioMin = 1\n",
    "    elif np.sign(MinA) == np.sign(MinB):\n",
    "        RatioMin = min(abs(MinA), abs(MinB)) / max(abs(MinA), abs(MinB))\n",
    "    else:\n",
    "        RatioMin = 0\n",
    "\n",
    "    Range = MaxOutcome - min(MinA, MinB)\n",
    "    trivial = CPC15_isStochasticDom(DistA, DistB)\n",
    "    BEVa = np.matrix.dot(DistA[:, 0], DistA[:, 1])\n",
    "    if ambiguous:\n",
    "        UEVb = np.matrix.dot(DistB[:, 0], np.repeat([1 / nB], nB))\n",
    "        BEVb = (1-psi) * (UEVb+BEVa) / 2 + psi * MinB\n",
    "        pEstB = np.repeat([float(nB)], 1)  # estimation of probabilties in Amb\n",
    "        t_SPminb = (BEVb - np.mean(DistB[1:nB+1, 0])) / (MinB - np.mean(DistB[1:nB+1, 0]))\n",
    "        if t_SPminb < 0:\n",
    "            pEstB[0] = 0\n",
    "        elif t_SPminb > 1:\n",
    "            pEstB[0] = 1\n",
    "        else:\n",
    "            pEstB[0] = t_SPminb\n",
    "\n",
    "        # Add nb-1 rows to pEstB:\n",
    "        pEstB = np.append(pEstB, np.repeat((1 - pEstB[0]) / (nB - 1), nB-1))\n",
    "\n",
    "    else:\n",
    "        pEstB = DistB[:, 1]\n",
    "        BEVb = np.matrix.dot(DistB[:, 0], pEstB)\n",
    "\n",
    "    # simulation of decisions\n",
    "    for trial in range(nTrials):\n",
    "        STa = 0\n",
    "        STb = 0\n",
    "        # mental simulations\n",
    "        for s in range(1, kapa[0]+1):\n",
    "            rndNum = np.random.uniform(size=2)\n",
    "            if rndNum[0] > pBias[nfeed]:  # Unbiased technique\n",
    "                if nfeed == 0:\n",
    "                    outcomeA = distSample(DistA[:, 0], DistA[:, 1], rndNum[1])\n",
    "                    outcomeB = distSample(DistB[:, 0], pEstB, rndNum[1])\n",
    "                else:\n",
    "                    uniprobs = np.repeat([1 / nfeed], nfeed)\n",
    "                    outcomeA = distSample(ObsPay[0:nfeed, 0], uniprobs, rndNum[1])\n",
    "                    outcomeB = distSample(ObsPay[0:nfeed, 1], uniprobs, rndNum[1])\n",
    "\n",
    "            elif rndNum[0] > (2 / 3) * pBias[nfeed]:  # uniform\n",
    "                outcomeA = distSample(DistA[:, 0], np.repeat([1 / nA], nA), rndNum[1])\n",
    "                outcomeB = distSample(DistB[:, 0], np.repeat([1 / nB], nB), rndNum[1])\n",
    "\n",
    "            elif rndNum[0] > (1 / 3) * pBias[nfeed]:  # contingent pessimism\n",
    "                if SignMax > 0 and RatioMin < gama:\n",
    "                    outcomeA = MinA\n",
    "                    outcomeB = MinB\n",
    "                else:\n",
    "                    outcomeA = distSample(DistA[:, 0], np.repeat([1 / nA], nA), rndNum[1])\n",
    "                    outcomeB = distSample(DistB[:, 0], np.repeat([1 / nB], nB), rndNum[1])\n",
    "\n",
    "            else:  # Sign\n",
    "                if nfeed == 0:\n",
    "                    outcomeA = Range * distSample(np.sign(DistA[:, 0]), DistA[:, 1], rndNum[1])\n",
    "                    outcomeB = Range * distSample(np.sign(DistB[:, 0]), pEstB, rndNum[1])\n",
    "                else:\n",
    "                    uniprobs = np.repeat([1 / nfeed], nfeed)\n",
    "                    outcomeA = Range * distSample(np.sign(ObsPay[0:nfeed, 0]), uniprobs, rndNum[1])\n",
    "                    outcomeB = Range * distSample(np.sign(ObsPay[0:nfeed, 1]), uniprobs, rndNum[1])\n",
    "\n",
    "            STa = STa + outcomeA\n",
    "            STb = STb + outcomeB\n",
    "\n",
    "        STa = STa / kapa\n",
    "        STb = STb / kapa\n",
    "\n",
    "        # error term\n",
    "        if trivial['dom'][0]:\n",
    "            error = 0\n",
    "        else:\n",
    "            error = sigma * np.random.normal(size=1)  # positive values contribute to attraction to A\n",
    "\n",
    "        # decision\n",
    "        Decision[trial] = (BEVa - BEVb) + (STa - STb) + error < 0\n",
    "        if (BEVa - BEVb) + (STa - STb) + error == 0:\n",
    "            Decision[trial] = np.random.choice(range(1, 3), size=1, replace=False) - 1\n",
    "\n",
    "        if trial >= firstFeedback - 1:\n",
    "            #  got feedback\n",
    "            nfeed += 1\n",
    "            pBias = np.append(pBias, beta / (beta + 1 + pow(nfeed, theta)))\n",
    "            rndNumObs = np.random.uniform(size=1)\n",
    "            ObsPay[nfeed - 1, 0] = distSample(DistA[:, 0], DistA[:, 1], rndNumObs)  # draw outcome from A\n",
    "            if Corr == 1:\n",
    "                ObsPay[nfeed - 1, 1] = distSample(DistB[:, 0], DistB[:, 1], rndNumObs)\n",
    "            elif Corr == -1:\n",
    "                ObsPay[nfeed - 1, 1] = distSample(DistB[:, 0], DistB[:, 1], 1-rndNumObs)\n",
    "            else:\n",
    "                # draw outcome from B\n",
    "                ObsPay[nfeed - 1, 1] = distSample(DistB[:, 0], DistB[:, 1], np.random.uniform(size=1))\n",
    "            if ambiguous:\n",
    "                BEVb = (1 - 1 / (nTrials-firstFeedback+1)) * BEVb + 1 / (nTrials-firstFeedback+1) * ObsPay[nfeed - 1, 1]\n",
    "\n",
    "    # compute B-rates for this simulation\n",
    "    blockSize = nTrials / nBlocks\n",
    "    for b in range(1, nBlocks+1):\n",
    "        simPred[b-1] = np.mean(Decision[int(((b - 1) * blockSize + 1)-1):int(b * blockSize)])\n",
    "\n",
    "    return simPred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CPC15_BEASTpred(Ha, pHa, La, LotShapeA, LotNumA, Hb, pHb, Lb, LotShapeB, LotNumB, Amb, Corr):\n",
    "    # Prediction of (the original) BEAST model for one problem\n",
    "    # Input: for a and b: high outcome (Ha/ Hb: int), its probability (pHa/ pHb: double), low outcome\n",
    "    #  (La/ Lb: int), the shape of the lottery (LotShapeA/ LotShapeB that can be:'-'/'Symm'/'L-skew'/'R-skew' only),\n",
    "    #  the number of outcomes in the lottery (lot_numA/ LotNumB: int),\n",
    "    #  Amb indicates if B is ambiguous (=1) or not (=0).\n",
    "    #  Corr is the correlation between A and B, this is a number between -1 to 1.\n",
    "    # Output: is the prediction of the BEAST model: this is a numpy of size (5,1)\n",
    "\n",
    "    Prediction = np.repeat([0], 5)\n",
    "    Prediction.shape = (5, 1)\n",
    "\n",
    "    # get both options' distributions\n",
    "    DistA = CPC18_getDist(Ha, pHa, La, LotShapeA, LotNumA)\n",
    "    DistB = CPC18_getDist(Hb, pHb, Lb, LotShapeB, LotNumB)\n",
    "\n",
    "    # run model simulation nSims times\n",
    "    nSims = 4000\n",
    "    for sim in range(0, nSims):\n",
    "        simPred = CPC15_BEASTsimulation(DistA, DistB, Amb, Corr)\n",
    "        Prediction = np.add(Prediction, (1 / nSims) * simPred)\n",
    "\n",
    "    return Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PF_Features(Ha, pHa, La, LotShapeA, LotNumA, Hb, pHb, Lb, LotShapeB, LotNumB, Amb, Corr):\n",
    "    # Finds the values of the engineered features that are part of Psychological Forest\n",
    "    # Gets as input the parameters defining the choice problem in CPC18 and returns\n",
    "    # as output a pandas data frame with this problem's features\n",
    "\n",
    "    # Compute \"naive\" and \"psychological\" features as per Plonsky, Erev, Hazan, and Tennenholtz, 2017\n",
    "    DistA = CPC18_getDist(Ha, pHa, La, LotShapeA, LotNumA)\n",
    "    DistB = CPC18_getDist(Hb, pHb, Lb, LotShapeB, LotNumB)\n",
    "    diffEV = (np.matrix.dot(DistB[:, 0], DistB[:, 1]) - np.matrix.dot(DistA[:, 0], DistA[:, 1]))\n",
    "    diffSDs = (getSD(DistB[:, 0], DistB[:, 1]) - getSD(DistA[:, 0], DistA[:, 1]))\n",
    "    MinA = DistA[0, 0]\n",
    "    MinB = DistB[0, 0]\n",
    "    diffMins = MinB - MinA\n",
    "    nA = DistA.shape[0]  # num outcomes in A\n",
    "    nB = DistB.shape[0]  # num outcomes in B\n",
    "    MaxA = DistA[nA - 1, 0]\n",
    "    MaxB = DistB[nB - 1, 0]\n",
    "    diffMaxs = MaxB - MaxA\n",
    "\n",
    "    diffUV = (np.matrix.dot(DistB[:, 0], np.repeat([1 / nB], nB))) - (np.matrix.dot(DistA[:, 0], np.repeat([1 / nA], nA)))\n",
    "    if Amb == 1:\n",
    "        ambiguous = True\n",
    "    else:\n",
    "        ambiguous = False\n",
    "\n",
    "    MaxOutcome = max(MaxA, MaxB)\n",
    "    SignMax = np.sign(MaxOutcome)\n",
    "    if MinA == MinB:\n",
    "        RatioMin = 1\n",
    "    elif np.sign(MinA) == np.sign(MinB):\n",
    "        RatioMin = min(abs(MinA), abs(MinB)) / max(abs(MinA), abs(MinB))\n",
    "    else:\n",
    "        RatioMin = 0\n",
    "\n",
    "    Range = MaxOutcome - min(MinA, MinB)\n",
    "    diffSignEV = (Range * np.matrix.dot(np.sign(DistB[:, 0]), DistB[:, 1]) -\n",
    "                  Range * np.matrix.dot(np.sign(DistA[:, 0]), DistA[:, 1]))\n",
    "    trivial = CPC15_isStochasticDom(DistA, DistB)\n",
    "    whchdom = trivial['which'][0]\n",
    "    Dom = 0\n",
    "    if trivial['dom'][0] and whchdom == 'A':\n",
    "        Dom = -1\n",
    "    if trivial['dom'][0] and whchdom == 'B':\n",
    "        Dom = 1\n",
    "    BEVa = np.matrix.dot(DistA[:, 0], DistA[:, 1])\n",
    "    if ambiguous:\n",
    "        UEVb = np.matrix.dot(DistB[:, 0], np.repeat(1 / nB, nB))\n",
    "        BEVb = (UEVb + BEVa + MinB) / 3\n",
    "        pEstB = np.repeat([float(nB)], 1)  # estimation of probabilties in Amb\n",
    "        t_SPminb = (BEVb - np.mean(DistB[1:nB + 1, 0])) / (MinB - np.mean(DistB[1:nB + 1, 0]))\n",
    "        if t_SPminb < 0:\n",
    "            pEstB[0] = 0\n",
    "        elif t_SPminb > 1:\n",
    "            pEstB[0] = 1\n",
    "        else:\n",
    "            pEstB[0] = t_SPminb\n",
    "        pEstB = np.append(pEstB, np.repeat([(1 - pEstB[0]) / (nB - 1)], nB - 1))\n",
    "    else:\n",
    "        pEstB = DistB[:, 1]\n",
    "        BEVb = np.matrix.dot(DistB[:, 0], pEstB)\n",
    "\n",
    "    diffBEV0 = (BEVb - BEVa)\n",
    "    BEVfb = (BEVb + (np.matrix.dot(DistB[:, 0], DistB[:, 1]))) / 2\n",
    "    diffBEVfb = (BEVfb - BEVa)\n",
    "\n",
    "    sampleDistB = np.column_stack((DistB[:, 0], pEstB))\n",
    "    probsBetter = get_pBetter(DistA, sampleDistB, corr=1)\n",
    "    pAbetter = probsBetter[0]\n",
    "    pBbetter = probsBetter[1]\n",
    "    pBbet_Unbiased1 = pBbetter - pAbetter\n",
    "\n",
    "    sampleUniDistA = np.column_stack((DistA[:, 0], np.repeat([1 / nA], nA)))\n",
    "    sampleUniDistB = np.column_stack((DistB[:, 0], np.repeat([1 / nB], nB)))\n",
    "    probsBetterUni = get_pBetter(sampleUniDistA, sampleUniDistB, corr=1)\n",
    "    pBbet_Uniform = probsBetterUni[1] - probsBetterUni[0]\n",
    "\n",
    "    sampleSignA = np.copy(DistA)\n",
    "    sampleSignA[:, 0] = np.sign(sampleSignA[:, 0])\n",
    "    sampleSignB = np.column_stack((np.sign(DistB[:, 0]), pEstB))\n",
    "    probsBetterSign = get_pBetter(sampleSignA, sampleSignB, corr=1)\n",
    "    pBbet_Sign1 = probsBetterSign[1] - probsBetterSign[0]\n",
    "    sampleSignBFB = np.column_stack((np.sign(DistB[:, 0]), DistB[:, 1]))\n",
    "    if Corr == 1:\n",
    "        probsBetter = get_pBetter(DistA, DistB, corr=1)\n",
    "        probsBetterSign = get_pBetter(sampleSignA, sampleSignBFB, corr=1)\n",
    "    elif Corr == -1:\n",
    "        probsBetter = get_pBetter(DistA, DistB, corr=-1)\n",
    "        probsBetterSign = get_pBetter(sampleSignA, sampleSignBFB, corr=-1)\n",
    "    else:\n",
    "        probsBetter = get_pBetter(DistA, DistB, corr=0)\n",
    "        probsBetterSign = get_pBetter(sampleSignA, sampleSignBFB, corr=0)\n",
    "\n",
    "    pBbet_UnbiasedFB = probsBetter[1] - probsBetter[0]\n",
    "    pBbet_SignFB = probsBetterSign[1] - probsBetterSign[0]\n",
    "\n",
    "    # convert lot shape: '-'/'Symm'/'L-skew'/'R-skew' to 4 different features for the RF model\n",
    "    lot_shape_listA = lot_shape_convert(LotShapeA)\n",
    "    lot_shape_listB = lot_shape_convert(LotShapeB)\n",
    "\n",
    "    # create features data frame\n",
    "    feats_labels = ('Ha', 'pHa', 'La', 'lot_shape__A', 'lot_shape_symm_A', 'lot_shape_L_A', 'lot_shape_R_A', 'LotNumA',\n",
    "                    'Hb', 'pHb', 'Lb', 'lot_shape__B', 'lot_shape_symm_B', 'lot_shape_L_B', 'lot_shape_R_B', 'LotNumB',\n",
    "                    'Amb', 'Corr', 'diffEV', 'diffSDs', 'diffMins', 'diffMaxs', 'diffUV', 'RatioMin', 'SignMax',\n",
    "                    'pBbet_Unbiased1', 'pBbet_UnbiasedFB', 'pBbet_Uniform', 'pBbet_Sign1', 'pBbet_SignFB', 'Dom',\n",
    "                    'diffBEV0', 'diffBEVfb', 'diffSignEV')\n",
    "    data_lists = [[Ha, pHa, La], lot_shape_listA, [LotNumA, Hb, pHb, Lb], lot_shape_listB, [LotNumB, Amb, Corr,\n",
    "                             diffEV, diffSDs, diffMins, diffMaxs, diffUV, RatioMin, SignMax, pBbet_Unbiased1,\n",
    "                             pBbet_UnbiasedFB, pBbet_Uniform, pBbet_Sign1, pBbet_SignFB, Dom, diffBEV0,\n",
    "                             diffBEVfb, diffSignEV]]\n",
    "    features_data = [item for sublist in data_lists for item in sublist]\n",
    "    tmpFeats = pd.DataFrame(features_data, index=feats_labels).T\n",
    "\n",
    "    # duplicate features data frame as per number of blocks\n",
    "    Feats = pd.concat([tmpFeats] * 5)\n",
    "\n",
    "    # get BEAST model prediction as feature\n",
    "    beastPs = CPC15_BEASTpred(Ha, pHa, La, LotShapeA, LotNumA, Hb, pHb, Lb, LotShapeB, LotNumB, Amb, Corr)\n",
    "    Feats['BEASTpred'] = beastPs\n",
    "\n",
    "    Feats['block'] = np.arange(1, 6)\n",
    "    Feats['Feedback'] = 1\n",
    "    Feats.loc[Feats['block'] == 1, 'Feedback'] = 0\n",
    "\n",
    "    return Feats\n",
    "\n",
    "\n",
    "# To compute the distribution's standard deviation\n",
    "def getSD(vals, probs):\n",
    "    m = np.matrix.dot(vals, probs.T)\n",
    "    sqds = np.power((vals - m), 2)\n",
    "    var = np.matrix.dot(probs, sqds.T)\n",
    "    return math.sqrt(var)\n",
    "\n",
    "\n",
    "# Convert lot shape feautre to vector of 4 features\n",
    "def lot_shape_convert(lot_shape):\n",
    "    return {\n",
    "        '-': [1, 0, 0, 0],\n",
    "        'Symm': [0, 1, 0, 0],\n",
    "        'L-skew': [0, 0, 1, 0],\n",
    "        'R-skew': [0, 0, 0, 1],\n",
    "    }[lot_shape]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Model:\n",
    "I have used an MLPRegressor here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CPC18_PF_pred(train_data, Ha, pHa, La, LotShapeA, LotNumA, Hb, pHb, Lb, LotShapeB, LotNumB, Amb, Corr):\n",
    "    Feats = get_PF_Features(Ha, pHa, La, LotShapeA, LotNumA, Hb, pHb, Lb, LotShapeB, LotNumB, Amb, Corr)\n",
    "    n_runs = 10\n",
    "    prediction = np.repeat([0], 5)\n",
    "    prediction.shape = (1, 5)\n",
    "    for run in range(n_runs):\n",
    "\n",
    "        x_train = train_data.iloc[:, 1:38]\n",
    "        y_train = train_data['B_rate']\n",
    "        mlp_model =  MLPRegressor(max_iter=500, hidden_layer_sizes=[200, 275, 100])\n",
    "        mlp_model.fit(X=x_train, y=y_train)\n",
    "\n",
    "        pred = mlp_model.predict(Feats)\n",
    "        prediction = np.add(prediction, (1 / n_runs) * pred)\n",
    "\n",
    "    return prediction, mlp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "### Section B: Please do not change this section ###\n",
    "####################################################\n",
    "# load problems to predict (in this example, the estimation set problems)\n",
    "Data = pd.read_csv('CPC18_EstSet.csv')\n",
    "train_data = pd.read_csv('TrainData.csv')\n",
    "# useful variables\n",
    "nProblems = Data.shape[0]\n",
    "PredictedAll = np.zeros(shape=(nProblems, 5))\n",
    "### End of Section A ###\n",
    "#################################################################\n",
    "### Section C: Please change only lines 41-47 in this section ###\n",
    "#################################################################\n",
    "for prob in range(nProblems):\n",
    "    # read problem's parameters\n",
    "    Ha = Data['Ha'][prob]\n",
    "    pHa = Data['pHa'][prob]\n",
    "    La = Data['La'][prob]\n",
    "    LotShapeA = Data['LotShapeA'][prob]\n",
    "    LotNumA = Data['LotNumA'][prob]\n",
    "    Hb = Data['Hb'][prob]\n",
    "    pHb = Data['pHb'][prob]\n",
    "    Lb = Data['Lb'][prob]\n",
    "    LotShapeB = Data['LotShapeB'][prob]\n",
    "    LotNumB = Data['LotNumB'][prob]\n",
    "    Amb = Data['Amb'][prob]\n",
    "    Corr = Data['Corr'][prob]\n",
    "    # new model\n",
    "    Prediction, model = CPC18_PF_pred(train_data, Ha, pHa, La, LotShapeA, LotNumA, Hb, pHb, Lb, LotShapeB, LotNumB, Amb,\n",
    "                               Corr)\n",
    "    # end of example\n",
    "    PredictedAll[prob, :] = Prediction\n",
    "    # for verbose progression\n",
    "    print('{}: Finish problem number: {}'.format((time.asctime(time.localtime(time.time()))), prob + 1))\n",
    "    # pd.DataFrame(model.loss_curve_).plot()\n",
    "### End of Section C ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE over the 60 problems: 9.034410759679913\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "### Section D: Please do not change this section ###\n",
    "####################################################\n",
    "# compute MSE\n",
    "ObservedAll = Data[['B.1', 'B.2', 'B.3', 'B.4', 'B.5']]\n",
    "probMSEs = 100 * ((PredictedAll - ObservedAll) ** 2).mean(axis=1)\n",
    "totalMSE = np.mean(probMSEs)\n",
    "print('MSE over the {} problems: {}'.format(nProblems, totalMSE))\n",
    "# for keeping the predicted choice rates\n",
    "np.savetxt(\"outputAll_MLP.csv\", PredictedAll, delimiter=\",\", header=\"B1,B2,B3,B4,B5\", fmt='%.4e')\n",
    "### End of Section D ###\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification ( same modification as in the baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CPC18_PF_pred_new(train_data, Ha, pHa, La, LotShapeA, LotNumA, Hb, pHb, Lb, LotShapeB, LotNumB, Amb, Corr, mlp_model):\n",
    "    Feats = get_PF_Features(Ha, pHa, La, LotShapeA, LotNumA, Hb, pHb, Lb, LotShapeB, LotNumB, Amb, Corr)\n",
    "    \n",
    "    n_runs = 10\n",
    "    prediction = np.repeat([0], 5)\n",
    "    prediction.shape = (1, 5)\n",
    "    for run in range(n_runs):\n",
    "        x_train = train_data.iloc[:, 1:38]\n",
    "        y_train = train_data['B_rate']\n",
    "        mlp_model.fit(X=x_train, y=y_train)\n",
    "        pred = mlp_model.predict(Feats)\n",
    "        # errors.append(log_loss(y_train, pred))\n",
    "        prediction = np.add(prediction, (1 / n_runs) * pred)\n",
    "\n",
    "    return prediction, mlp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May  6 02:46:36 2023: Finish problem number: 1\n",
      "Sat May  6 02:46:49 2023: Finish problem number: 2\n",
      "Sat May  6 02:47:00 2023: Finish problem number: 3\n",
      "Sat May  6 02:47:12 2023: Finish problem number: 4\n",
      "Sat May  6 02:47:24 2023: Finish problem number: 5\n",
      "Sat May  6 02:47:36 2023: Finish problem number: 6\n",
      "Sat May  6 02:47:48 2023: Finish problem number: 7\n",
      "Sat May  6 02:48:01 2023: Finish problem number: 8\n",
      "Sat May  6 02:48:13 2023: Finish problem number: 9\n",
      "Sat May  6 02:48:27 2023: Finish problem number: 10\n",
      "Sat May  6 02:48:41 2023: Finish problem number: 11\n",
      "Sat May  6 02:48:53 2023: Finish problem number: 12\n",
      "Sat May  6 02:49:05 2023: Finish problem number: 13\n",
      "Sat May  6 02:49:16 2023: Finish problem number: 14\n",
      "Sat May  6 02:49:28 2023: Finish problem number: 15\n",
      "Sat May  6 02:49:39 2023: Finish problem number: 16\n",
      "Sat May  6 02:49:50 2023: Finish problem number: 17\n",
      "Sat May  6 02:50:03 2023: Finish problem number: 18\n",
      "Sat May  6 02:50:14 2023: Finish problem number: 19\n",
      "Sat May  6 02:50:26 2023: Finish problem number: 20\n",
      "Sat May  6 02:50:37 2023: Finish problem number: 21\n",
      "Sat May  6 02:50:48 2023: Finish problem number: 22\n",
      "Sat May  6 02:50:59 2023: Finish problem number: 23\n",
      "Sat May  6 02:51:12 2023: Finish problem number: 24\n",
      "Sat May  6 02:51:26 2023: Finish problem number: 25\n",
      "Sat May  6 02:51:40 2023: Finish problem number: 26\n",
      "Sat May  6 02:51:55 2023: Finish problem number: 27\n",
      "Sat May  6 02:52:09 2023: Finish problem number: 28\n",
      "Sat May  6 02:52:21 2023: Finish problem number: 29\n",
      "Sat May  6 02:52:33 2023: Finish problem number: 30\n",
      "Sat May  6 02:52:46 2023: Finish problem number: 31\n",
      "Sat May  6 02:52:58 2023: Finish problem number: 32\n",
      "Sat May  6 02:53:10 2023: Finish problem number: 33\n",
      "Sat May  6 02:53:24 2023: Finish problem number: 34\n",
      "Sat May  6 02:53:37 2023: Finish problem number: 35\n",
      "Sat May  6 02:53:51 2023: Finish problem number: 36\n",
      "Sat May  6 02:54:03 2023: Finish problem number: 37\n",
      "Sat May  6 02:54:13 2023: Finish problem number: 38\n",
      "Sat May  6 02:54:24 2023: Finish problem number: 39\n",
      "Sat May  6 02:54:35 2023: Finish problem number: 40\n",
      "Sat May  6 02:54:45 2023: Finish problem number: 41\n",
      "Sat May  6 02:54:57 2023: Finish problem number: 42\n",
      "Sat May  6 02:55:08 2023: Finish problem number: 43\n",
      "Sat May  6 02:55:20 2023: Finish problem number: 44\n",
      "Sat May  6 02:55:33 2023: Finish problem number: 45\n",
      "Sat May  6 02:55:45 2023: Finish problem number: 46\n",
      "Sat May  6 02:55:57 2023: Finish problem number: 47\n",
      "Sat May  6 02:56:09 2023: Finish problem number: 48\n",
      "Sat May  6 02:56:21 2023: Finish problem number: 49\n",
      "Sat May  6 02:56:32 2023: Finish problem number: 50\n",
      "Sat May  6 02:56:43 2023: Finish problem number: 51\n",
      "Sat May  6 02:56:54 2023: Finish problem number: 52\n",
      "Sat May  6 02:57:05 2023: Finish problem number: 53\n",
      "Sat May  6 02:57:17 2023: Finish problem number: 54\n",
      "Sat May  6 02:57:30 2023: Finish problem number: 55\n",
      "Sat May  6 02:57:44 2023: Finish problem number: 56\n",
      "Sat May  6 02:57:57 2023: Finish problem number: 57\n",
      "Sat May  6 02:58:10 2023: Finish problem number: 58\n",
      "Sat May  6 02:58:23 2023: Finish problem number: 59\n",
      "Sat May  6 02:58:36 2023: Finish problem number: 60\n",
      "MSE over the 60 problems: 5.596049896764009\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    Data = pd.read_csv('CPC18_EstSet.csv')\n",
    "    train_data = pd.read_csv('TrainData.csv')\n",
    "    nProblems = Data.shape[0]\n",
    "    PredictedAll = np.zeros(shape=(nProblems, 5))\n",
    "    \n",
    "    # errors = []\n",
    "    \n",
    "    mlp_model = MLPRegressor(max_iter=500, hidden_layer_sizes=[200, 275, 100], warm_start=True)\n",
    "    for prob in range(nProblems):\n",
    "        # read problem's parameters\n",
    "        Ha = Data['Ha'][prob]\n",
    "        pHa = Data['pHa'][prob]\n",
    "        La = Data['La'][prob]\n",
    "        LotShapeA = Data['LotShapeA'][prob]\n",
    "        LotNumA = Data['LotNumA'][prob]\n",
    "        Hb = Data['Hb'][prob]\n",
    "        pHb = Data['pHb'][prob]\n",
    "        Lb = Data['Lb'][prob]\n",
    "        LotShapeB = Data['LotShapeB'][prob]\n",
    "        LotNumB = Data['LotNumB'][prob]\n",
    "        Amb = Data['Amb'][prob]\n",
    "        Corr = Data['Corr'][prob]\n",
    "\n",
    "\n",
    "        Prediction, mlp_model = CPC18_PF_pred_new(train_data, Ha, pHa, La, LotShapeA, LotNumA, Hb, pHb, Lb, LotShapeB, LotNumB, Amb,\n",
    "                                   Corr, mlp_model)\n",
    "\n",
    "        PredictedAll[prob, :] = Prediction\n",
    "        print('{}: Finish problem number: {}'.format((time.asctime(time.localtime(time.time()))), prob + 1))\n",
    "\n",
    "  \n",
    "\n",
    "ObservedAll = Data[['B.1', 'B.2', 'B.3', 'B.4', 'B.5']]\n",
    "probMSEs = 100 * ((PredictedAll - ObservedAll) ** 2).mean(axis=1)\n",
    "totalMSE = np.mean(probMSEs)\n",
    "print('MSE over the {} problems: {}'.format(nProblems, totalMSE))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Modification improves the MSE but it's still not as good as the baseline.\n",
    "\n",
    "\n",
    "#### I further increase the number of hidden layers and the number of units in each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May  6 03:37:40 2023: Finish problem number: 1\n",
      "Sat May  6 03:37:55 2023: Finish problem number: 2\n",
      "Sat May  6 03:38:09 2023: Finish problem number: 3\n",
      "Sat May  6 03:38:22 2023: Finish problem number: 4\n",
      "Sat May  6 03:38:35 2023: Finish problem number: 5\n",
      "Sat May  6 03:38:49 2023: Finish problem number: 6\n",
      "Sat May  6 03:39:02 2023: Finish problem number: 7\n",
      "Sat May  6 03:39:15 2023: Finish problem number: 8\n",
      "Sat May  6 03:39:27 2023: Finish problem number: 9\n",
      "Sat May  6 03:39:40 2023: Finish problem number: 10\n",
      "Sat May  6 03:39:54 2023: Finish problem number: 11\n",
      "Sat May  6 03:40:09 2023: Finish problem number: 12\n",
      "Sat May  6 03:40:23 2023: Finish problem number: 13\n",
      "Sat May  6 03:40:38 2023: Finish problem number: 14\n",
      "Sat May  6 03:40:49 2023: Finish problem number: 15\n",
      "Sat May  6 03:41:02 2023: Finish problem number: 16\n",
      "Sat May  6 03:41:16 2023: Finish problem number: 17\n",
      "Sat May  6 03:41:31 2023: Finish problem number: 18\n",
      "Sat May  6 03:41:46 2023: Finish problem number: 19\n",
      "Sat May  6 03:42:02 2023: Finish problem number: 20\n",
      "Sat May  6 03:42:19 2023: Finish problem number: 21\n",
      "Sat May  6 03:42:37 2023: Finish problem number: 22\n",
      "Sat May  6 03:42:53 2023: Finish problem number: 23\n",
      "Sat May  6 03:43:10 2023: Finish problem number: 24\n",
      "Sat May  6 03:43:28 2023: Finish problem number: 25\n",
      "Sat May  6 03:43:45 2023: Finish problem number: 26\n",
      "Sat May  6 03:44:03 2023: Finish problem number: 27\n",
      "Sat May  6 03:44:17 2023: Finish problem number: 28\n",
      "Sat May  6 03:44:32 2023: Finish problem number: 29\n",
      "Sat May  6 03:44:47 2023: Finish problem number: 30\n",
      "Sat May  6 03:44:59 2023: Finish problem number: 31\n",
      "Sat May  6 03:45:11 2023: Finish problem number: 32\n",
      "Sat May  6 03:45:23 2023: Finish problem number: 33\n",
      "Sat May  6 03:45:35 2023: Finish problem number: 34\n",
      "Sat May  6 03:45:48 2023: Finish problem number: 35\n",
      "Sat May  6 03:46:02 2023: Finish problem number: 36\n",
      "Sat May  6 03:46:15 2023: Finish problem number: 37\n",
      "Sat May  6 03:46:28 2023: Finish problem number: 38\n",
      "Sat May  6 03:46:43 2023: Finish problem number: 39\n",
      "Sat May  6 03:46:56 2023: Finish problem number: 40\n",
      "Sat May  6 03:47:09 2023: Finish problem number: 41\n",
      "Sat May  6 03:47:21 2023: Finish problem number: 42\n",
      "Sat May  6 03:47:34 2023: Finish problem number: 43\n",
      "Sat May  6 03:47:47 2023: Finish problem number: 44\n",
      "Sat May  6 03:48:01 2023: Finish problem number: 45\n",
      "Sat May  6 03:48:14 2023: Finish problem number: 46\n",
      "Sat May  6 03:48:27 2023: Finish problem number: 47\n",
      "Sat May  6 03:48:42 2023: Finish problem number: 48\n",
      "Sat May  6 03:48:55 2023: Finish problem number: 49\n",
      "Sat May  6 03:49:07 2023: Finish problem number: 50\n",
      "Sat May  6 03:49:19 2023: Finish problem number: 51\n",
      "Sat May  6 03:49:31 2023: Finish problem number: 52\n",
      "Sat May  6 03:49:44 2023: Finish problem number: 53\n",
      "Sat May  6 03:49:57 2023: Finish problem number: 54\n",
      "Sat May  6 03:50:09 2023: Finish problem number: 55\n",
      "Sat May  6 03:50:23 2023: Finish problem number: 56\n",
      "Sat May  6 03:50:36 2023: Finish problem number: 57\n",
      "Sat May  6 03:50:49 2023: Finish problem number: 58\n",
      "Sat May  6 03:51:03 2023: Finish problem number: 59\n",
      "Sat May  6 03:51:17 2023: Finish problem number: 60\n",
      "MSE over the 60 problems: 2.0638094422805295\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    Data = pd.read_csv('CPC18_EstSet.csv')\n",
    "    train_data = pd.read_csv('TrainData.csv')\n",
    "    nProblems = Data.shape[0]\n",
    "    PredictedAll = np.zeros(shape=(nProblems, 5))\n",
    "    \n",
    "    # errors = []\n",
    "    \n",
    "    mlp_model = MLPRegressor(max_iter=1000, hidden_layer_sizes=[1024, 512, 256, 128, 64, 32], warm_start=True)\n",
    "    for prob in range(nProblems):\n",
    "        # read problem's parameters\n",
    "        Ha = Data['Ha'][prob]\n",
    "        pHa = Data['pHa'][prob]\n",
    "        La = Data['La'][prob]\n",
    "        LotShapeA = Data['LotShapeA'][prob]\n",
    "        LotNumA = Data['LotNumA'][prob]\n",
    "        Hb = Data['Hb'][prob]\n",
    "        pHb = Data['pHb'][prob]\n",
    "        Lb = Data['Lb'][prob]\n",
    "        LotShapeB = Data['LotShapeB'][prob]\n",
    "        LotNumB = Data['LotNumB'][prob]\n",
    "        Amb = Data['Amb'][prob]\n",
    "        Corr = Data['Corr'][prob]\n",
    "\n",
    "\n",
    "        Prediction, mlp_model = CPC18_PF_pred_new(train_data, Ha, pHa, La, LotShapeA, LotNumA, Hb, pHb, Lb, LotShapeB, LotNumB, Amb,\n",
    "                                   Corr, mlp_model)\n",
    "\n",
    "        PredictedAll[prob, :] = Prediction\n",
    "        print('{}: Finish problem number: {}'.format((time.asctime(time.localtime(time.time()))), prob + 1))\n",
    "\n",
    "  \n",
    "\n",
    "ObservedAll = Data[['B.1', 'B.2', 'B.3', 'B.4', 'B.5']]\n",
    "probMSEs = 100 * ((PredictedAll - ObservedAll) ** 2).mean(axis=1)\n",
    "totalMSE = np.mean(probMSEs)\n",
    "print('MSE over the {} problems: {}'.format(nProblems, totalMSE))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The MSE improves even further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
